{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDqBMS1KBMo2"
      },
      "source": [
        "# **Study Project:** *Transformer model for prediction of grasping movements*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC1eRfTGuWyA",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install pydrive\n",
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0yyZ3iVmk2a",
        "outputId": "f10d9cd5-c36b-4a8a-d5bf-badb608b674a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.207-py3-none-any.whl (645 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.2/645.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.207\n",
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.10/dist-packages (from pydrive) (2.84.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (6.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (4.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.2->pydrive) (5.3.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->pydrive) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F7Sn6IstgaA6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "\n",
        "from scipy import interpolate\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#!pip install ultralytics\n",
        "#ipd.clear_output()\n",
        "import ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Debugging\n",
        "import sys"
      ],
      "metadata": {
        "id": "-nfl24K63ZMC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJf8BcE-to4F",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LCgIq5tJPEHX"
      },
      "outputs": [],
      "source": [
        "def GetBoundingBoxes(videopath):\n",
        "    \"\"\"\n",
        "    Extract bounding boxes from video.\n",
        "\n",
        "    Args:\n",
        "    videopath -- path to video to extract bounding boxes from.\n",
        "    \"\"\"\n",
        "    # Use object tracker to get the bounding boxes and classIDs in a 'results' object\n",
        "    bbs = np.zeros((1, 7))\n",
        "    # It is also possible to pass the whole folder as path,\n",
        "    # but we still want the flexibility to access single videos\n",
        "    results = model.track(source=videopath, tracker=\"bytetrack.yaml\")\n",
        "\n",
        "    # Get class names\n",
        "    classes = results[0].names\n",
        "\n",
        "    # Iterate through each frame of a video to get all bounding boxes for a frame\n",
        "    for frame in range(len(results)):\n",
        "\n",
        "        # x_center, y_center, bbwidth, bbheight of bbs of this frame\n",
        "        xywh = results[frame].boxes.xywh.detach().cpu().numpy()\n",
        "        n = len(xywh) # number of bounding boxes\n",
        "        cls = results[frame].boxes.cls.detach().cpu().numpy().reshape((n,1))\n",
        "        # if the object tracker is currently tracking at least one object, save the trackingID for that object, else fill with -1 placeholder\n",
        "        trackingID = results[frame].boxes.id.detach().cpu().numpy().reshape((n,1)) if results[frame].boxes.is_track else np.repeat(-1, n).reshape((n,1))\n",
        "        frame_count = np.repeat(frame, n).reshape((n,1))\n",
        "\n",
        "        # bind the data together for one frame\n",
        "        data = np.concatenate((frame_count, cls, xywh, trackingID), axis=1)\n",
        "        # add all data of this frame to all data of this video\n",
        "        bbs = np.concatenate((bbs, data), axis=0)\n",
        "\n",
        "    return bbs, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KjTnqYuouFnu"
      },
      "outputs": [],
      "source": [
        "def Center(videopath, to_path, bounding_boxes, width=1920, height=1120):\n",
        "\n",
        "    \"\"\"\n",
        "    Create a video where a target object is always centered.\n",
        "\n",
        "    Args:\n",
        "    videopath -- path to video to center\n",
        "    to_path -- location to export centered video to\n",
        "    bounding_boxes -- bbs of object to center on\n",
        "    width -- output video dim x, default=1920\n",
        "    height -- output video dim y, default=1120\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Read video\n",
        "    cap = cv2.VideoCapture(videopath)\n",
        "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    video = np.zeros((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
        "    fc = 0\n",
        "    ret = True\n",
        "\n",
        "    while (fc < frameCount and ret):\n",
        "        ret, video[fc] = cap.read()\n",
        "        fc += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "    # 2. Correct video\n",
        "    bbs = bounding_boxes\n",
        "    width = width\n",
        "    height = height\n",
        "    center_x = (width/2)\n",
        "    center_y = (height/2)\n",
        "    x_dists = []\n",
        "    y_dists = []\n",
        "\n",
        "    for box_pos in bbs:\n",
        "        if np.isnan(box_pos).any():\n",
        "            # if all values were nan, then 0 would be the max, so the corrected video would be the original video\n",
        "            x_dists.append(0)\n",
        "            y_dists.append(0)\n",
        "        else:\n",
        "            # get center of bb\n",
        "            #center_x_bb = (box_pos[0] + box_pos[2]) / 2\n",
        "            #center_y_bb = (box_pos[1] + box_pos[3]) / 2\n",
        "            center_x_bb = box_pos[0]\n",
        "            center_y_bb = box_pos[1]\n",
        "            # calculate distances\n",
        "            x_dists.append(abs(center_x-center_x_bb))\n",
        "            y_dists.append(abs(center_y-center_y_bb))\n",
        "\n",
        "\n",
        "    video_corrected = np.zeros((len(video),\n",
        "                              int(height + max(y_dists)*2),\n",
        "                              int(width + max(x_dists)*2),\n",
        "                              3), dtype=int)\n",
        "\n",
        "    # 3. Center video\n",
        "    # get coords for placement height and width. may be switched\n",
        "    start_row = (video_corrected.shape[1] - height) // 2\n",
        "    start_col = (video_corrected.shape[2] - width) // 2\n",
        "\n",
        "    offsets = np.zeros(shape=(len(video),2))\n",
        "\n",
        "    for idx, frame in enumerate(video):\n",
        "\n",
        "        # get matching bb\n",
        "        box_curr = bbs[idx]\n",
        "\n",
        "        # If there is no information on fruit, color the frame black (skip iter)\n",
        "        if np.isnan(box_curr).any():\n",
        "            continue\n",
        "            # frame[:] = 0\n",
        "            # frame = np.zeros(frame.shape)\n",
        "        else:\n",
        "            # get center of bb\n",
        "            #center_x_bb = (box_curr[0] + box_curr[2]) / 2\n",
        "            #center_y_bb = (box_curr[1] + box_curr[3]) / 2\n",
        "            center_x_bb = box_curr[0]\n",
        "            center_y_bb = box_curr[1]\n",
        "\n",
        "            # get offset of center\n",
        "            # pos if bounding box is to right of center, else negative\n",
        "            x_offset = int(center_x_bb - center_x)\n",
        "            # pos if bounding box is below center, else negative\n",
        "            y_offset = int(center_y_bb - center_y)\n",
        "\n",
        "            #!coordinates until here are for old video\n",
        "\n",
        "            #get fitting indices for new video\n",
        "            fixed_start_row = start_row - y_offset\n",
        "            fixed_start_col = start_col - x_offset\n",
        "\n",
        "            fixed_end_row = fixed_start_row + height\n",
        "            fixed_end_col = fixed_start_col + width\n",
        "\n",
        "            # Checkup\n",
        "            if((fixed_start_row or\n",
        "                fixed_start_col or\n",
        "                fixed_end_row or\n",
        "                fixed_end_col) < 0):\n",
        "                print(\"Negative Index!\")\n",
        "\n",
        "            # Save centered + corrected in new video\n",
        "            video_corrected[idx][fixed_start_row:fixed_end_row,\n",
        "                              fixed_start_col:fixed_end_col] = frame\n",
        "\n",
        "            #Save offsets\n",
        "            offsets[idx,:] = x_offset, y_offset\n",
        "\n",
        "\n",
        "    # 4. Save video\n",
        "    video_corrected = np.uint8(video_corrected)\n",
        "\n",
        "    height_new = int(video_corrected.shape[1])\n",
        "    width_new = int(video_corrected.shape[2])\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(to_path[:-4]+'_centered.mp4', fourcc, FPS, (width_new, height_new), True)\n",
        "    for idx in range(len(video)):\n",
        "        out.write(video_corrected[idx])\n",
        "    out.release()\n",
        "\n",
        "    return offsets, (start_row, start_col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1OVMgLRiBSf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Video centering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gJT7yT2FnWFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948e4ebe-29da-4cd9-95a5-fe553e55fca9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Acess google drive with pydrive to safe BBs and Offsets\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Set the destination Google Drive folder ID. Folder Name: Centering_OffsetsAndBBs under Data\n",
        "offsets_folder_id = '1d6hcNnHVwsEexY2GDoHGCSLQZYZUbfcs'\n",
        "bbs_targets_folder_id = '1yS_eBVOe-f5R-UIsyyZiWiG37t0s5od2'\n",
        "bbs_hands_folder_id = '1mVdxyoVfBp6LU-KNgWwZ28NLDEn9KNf0'"
      ],
      "metadata": {
        "id": "KQIFJ6oGhxFR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FxE-wPP7SFe3"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = ultralytics.YOLO('yolov8n.pt')\n",
        "\n",
        "# List videos\n",
        "#folderpath = '/content/drive/MyDrive/Study_Project_Grasping_Copy/Transformer/Trials/'\n",
        "#labelpath = '/content/drive/MyDrive/Study_Project_Grasping_Copy/Transformer/Labels/'\n",
        "#folder = [f for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f))]\n",
        "folderpath = '/content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/'\n",
        "labelpath = '/content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Labels2/'\n",
        "folder = [f for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f))]\n",
        "\n",
        "# Example video\n",
        "#ipd.Video(folderpath+'banana02.mp4', width=1920/1.7, height=1120/1.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkb5TAAFGKWp"
      },
      "source": [
        "**Procedure:**\n",
        "1. Get the bounding boxes of all objects in each raw video.\n",
        "2. Filter for hand and target object bounding boxes.\n",
        "3. Target centering: fixate the target object (TARGET.mp4) using interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fiJg0H1RXt_v",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b88378c-7865-4b58-e280-8f2e7c45601b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video 1/1: banana02.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (1/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 bed, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 11.3ms\n",
            "video 1/1 (2/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 bed, 1 tv, 1 laptop, 1 keyboard, 7.5ms\n",
            "video 1/1 (3/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 bed, 1 tv, 1 laptop, 1 keyboard, 12.0ms\n",
            "video 1/1 (4/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 keyboard, 6.9ms\n",
            "video 1/1 (5/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 keyboard, 9.4ms\n",
            "video 1/1 (6/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 keyboard, 9.3ms\n",
            "video 1/1 (7/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 laptop, 1 keyboard, 2 cell phones, 10.3ms\n",
            "video 1/1 (8/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 tv, 1 laptop, 1 keyboard, 2 cell phones, 1 book, 8.5ms\n",
            "video 1/1 (9/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 tv, 1 laptop, 1 keyboard, 2 cell phones, 1 book, 10.6ms\n",
            "video 1/1 (10/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 2 cell phones, 2 books, 9.1ms\n",
            "video 1/1 (11/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 9.0ms\n",
            "video 1/1 (12/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 2 books, 8.9ms\n",
            "video 1/1 (13/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 1 microwave, 7.8ms\n",
            "video 1/1 (14/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 1 microwave, 8.3ms\n",
            "video 1/1 (15/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 keyboard, 2 cell phones, 1 book, 9.7ms\n",
            "video 1/1 (16/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 tv, 2 laptops, 1 keyboard, 1 cell phone, 7.3ms\n",
            "video 1/1 (17/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 tv, 2 laptops, 1 keyboard, 1 cell phone, 9.8ms\n",
            "video 1/1 (18/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 tvs, 2 laptops, 2 cell phones, 9.6ms\n",
            "video 1/1 (19/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 8.9ms\n",
            "video 1/1 (20/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 9.7ms\n",
            "video 1/1 (21/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 10.1ms\n",
            "video 1/1 (22/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 10.6ms\n",
            "video 1/1 (23/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 orange, 1 keyboard, 1 cell phone, 1 book, 9.5ms\n",
            "video 1/1 (24/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 10.8ms\n",
            "video 1/1 (25/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 keyboard, 1 cell phone, 11.6ms\n",
            "video 1/1 (26/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 apple, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 6.6ms\n",
            "video 1/1 (27/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 apple, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 9.6ms\n",
            "video 1/1 (28/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 8.7ms\n",
            "video 1/1 (29/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 10.8ms\n",
            "video 1/1 (30/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 7.2ms\n",
            "video 1/1 (31/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 8.7ms\n",
            "video 1/1 (32/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 9.2ms\n",
            "video 1/1 (33/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 cell phone, 7.2ms\n",
            "video 1/1 (34/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 cell phone, 9.2ms\n",
            "video 1/1 (35/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 2 tvs, 1 cell phone, 7.3ms\n",
            "video 1/1 (36/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 laptop, 1 cell phone, 7.4ms\n",
            "video 1/1 (37/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 laptop, 1 cell phone, 8.7ms\n",
            "video 1/1 (38/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 laptop, 1 cell phone, 9.4ms\n",
            "video 1/1 (39/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 10.6ms\n",
            "video 1/1 (40/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 8.2ms\n",
            "video 1/1 (41/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 10.4ms\n",
            "video 1/1 (42/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 11.3ms\n",
            "video 1/1 (43/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 10.2ms\n",
            "video 1/1 (44/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 9.5ms\n",
            "video 1/1 (45/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 9.1ms\n",
            "video 1/1 (46/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 9.9ms\n",
            "video 1/1 (47/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 8.2ms\n",
            "video 1/1 (48/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 10.5ms\n",
            "video 1/1 (49/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 9.2ms\n",
            "video 1/1 (50/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 tv, 1 laptop, 1 cell phone, 8.8ms\n",
            "video 1/1 (51/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 cell phone, 9.7ms\n",
            "video 1/1 (52/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 cell phone, 9.1ms\n",
            "video 1/1 (53/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 keyboard, 1 cell phone, 1 book, 6.7ms\n",
            "video 1/1 (54/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 keyboard, 1 cell phone, 9.3ms\n",
            "video 1/1 (55/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 2 tvs, 1 laptop, 1 keyboard, 1 cell phone, 8.3ms\n",
            "video 1/1 (56/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 2 tvs, 1 laptop, 1 keyboard, 1 cell phone, 9.5ms\n",
            "video 1/1 (57/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 10.3ms\n",
            "video 1/1 (58/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 1 book, 9.4ms\n",
            "video 1/1 (59/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 1 book, 9.7ms\n",
            "video 1/1 (60/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 keyboard, 1 cell phone, 1 book, 11.1ms\n",
            "video 1/1 (61/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 11.0ms\n",
            "video 1/1 (62/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 laptop, 1 cell phone, 1 book, 7.3ms\n",
            "video 1/1 (63/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 laptop, 1 cell phone, 9.4ms\n",
            "video 1/1 (64/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 1 book, 9.7ms\n",
            "video 1/1 (65/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 laptops, 1 cell phone, 10.4ms\n",
            "video 1/1 (66/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 7.8ms\n",
            "video 1/1 (67/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 11.5ms\n",
            "video 1/1 (68/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 2 books, 6.8ms\n",
            "video 1/1 (69/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 2 books, 10.5ms\n",
            "video 1/1 (70/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 2 laptops, 1 cell phone, 1 book, 7.2ms\n",
            "video 1/1 (71/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 2 laptops, 1 cell phone, 1 book, 10.3ms\n",
            "video 1/1 (72/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 1 cell phone, 1 book, 9.5ms\n",
            "video 1/1 (73/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 8.8ms\n",
            "video 1/1 (74/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 11.0ms\n",
            "video 1/1 (75/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 tv, 3 laptops, 1 keyboard, 1 cell phone, 1 book, 10.0ms\n",
            "video 1/1 (76/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 9.6ms\n",
            "video 1/1 (77/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 1 book, 11.6ms\n",
            "video 1/1 (78/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 7.0ms\n",
            "video 1/1 (79/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 8.0ms\n",
            "video 1/1 (80/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 11.7ms\n",
            "video 1/1 (81/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 7.1ms\n",
            "video 1/1 (82/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 8.4ms\n",
            "video 1/1 (83/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 9.6ms\n",
            "video 1/1 (84/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 keyboard, 1 cell phone, 11.0ms\n",
            "video 1/1 (85/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 9.3ms\n",
            "video 1/1 (86/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 keyboard, 1 cell phone, 10.0ms\n",
            "video 1/1 (87/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 keyboard, 1 cell phone, 6.7ms\n",
            "video 1/1 (88/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 keyboard, 1 cell phone, 10.8ms\n",
            "video 1/1 (89/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 keyboard, 6.6ms\n",
            "video 1/1 (90/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 keyboard, 10.2ms\n",
            "video 1/1 (91/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 7.7ms\n",
            "video 1/1 (92/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 9.6ms\n",
            "video 1/1 (93/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 6.9ms\n",
            "video 1/1 (94/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 9.9ms\n",
            "video 1/1 (95/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 20.2ms\n",
            "video 1/1 (96/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 12.7ms\n",
            "video 1/1 (97/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 9.8ms\n",
            "video 1/1 (98/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 8.2ms\n",
            "video 1/1 (99/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 10.2ms\n",
            "video 1/1 (100/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 6.5ms\n",
            "video 1/1 (101/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 10.2ms\n",
            "video 1/1 (102/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 9.1ms\n",
            "video 1/1 (103/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 9.6ms\n",
            "video 1/1 (104/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 cell phone, 6.6ms\n",
            "video 1/1 (105/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 cell phone, 9.5ms\n",
            "video 1/1 (106/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 8.1ms\n",
            "video 1/1 (107/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 cell phone, 12.1ms\n",
            "video 1/1 (108/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 cell phone, 9.8ms\n",
            "video 1/1 (109/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 11.0ms\n",
            "video 1/1 (110/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 8.6ms\n",
            "video 1/1 (111/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 9.9ms\n",
            "video 1/1 (112/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 6.8ms\n",
            "video 1/1 (113/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 11.2ms\n",
            "video 1/1 (114/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 9.0ms\n",
            "video 1/1 (115/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 10.0ms\n",
            "video 1/1 (116/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 7.0ms\n",
            "Speed: 3.6ms preprocess, 9.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "# Iterate through every video file in the folder\n",
        "for i, video in enumerate(folder):\n",
        "\n",
        "    print(f\"Video {i+1}/{len(folder)}: {video}\\n\")\n",
        "\n",
        "    # Get length of input array (number of frames)\n",
        "    cap = cv2.VideoCapture(folderpath+video)\n",
        "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "    # 1. Get all bbs of this video\n",
        "    # (x_center, y_center, bbwidth, bbheight, class, trackerID, frame, filename)\n",
        "    bbs, classes = GetBoundingBoxes(folderpath+video)\n",
        "    video_count = np.repeat(i, len(bbs)).reshape((len(bbs),1))\n",
        "    bbs = np.concatenate((video_count, bbs), axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Filter for hand and target fruit by class\n",
        "    # Get the target class to filter for as number\n",
        "    if '.mov' in video:\n",
        "        # returns the string name without '00.mp4' ending (could add try-catch)\n",
        "        target_class = video[:-6]\n",
        "    #added this, why was it .mov??\n",
        "    elif '.mp4' in video:\n",
        "        # returns the string name without '00.mp4' ending (could add try-catch)\n",
        "        target_class = video[:-6]\n",
        "\n",
        "\n",
        "    # Get class number from lookup table\n",
        "    target_class = list(classes.keys())[list(classes.values()).index(target_class)]\n",
        "    hand_class = list(classes.keys())[list(classes.values()).index('person')]\n",
        "\n",
        "    # Filter data\n",
        "    target_bbs = bbs[(bbs[:, 2] == target_class)]\n",
        "    hand_bbs = bbs[(bbs[:, 2] == hand_class)]\n",
        "\n",
        "    # To Do: Use trackerID to track the same object\n",
        "    # -> could be added later on, as we only use one object per trial?\n",
        "    # -> but if we have detection of more than one target fruit, it breaks something\n",
        "    # because it will try to calculate positions from more than one bb per frame.\n",
        "    # problem so far: if detection ends for one frame, a new trackerID is assigned\n",
        "    # -> for this approach to work we would have to compare the position of the\n",
        "    # last tracked trackerID to each object with newly assigned trackerID and\n",
        "    # take the closest one, and interpolate position if there are no new trackerIDs detected.\n",
        "    # for now, we leave it out: remove class and trackingID\n",
        "    target_bbs = target_bbs[:, [0,1,3,4,5,6]]\n",
        "    hand_bbs = hand_bbs[:, [0,1,3,4,5,6]]\n",
        "\n",
        "    # 3. Interpolation: take last known bounding box positions\n",
        "    # (x_center_fruit, y_center_fruit, bbwidth_fruit, bbheight_fruit, class, trackerID, frame, filename, x_center_hand, y_center_hand, bbwidth_hand, bbheight_hand)\n",
        "    vid_input = np.zeros((frameCount, 10))\n",
        "    vid_input[:] = np.nan # use as 'no information' instead of nan, because nan is a string\n",
        "\n",
        "\n",
        "    #Perform interpolation for targets\n",
        "\n",
        "    #get x values for which detection was successful\n",
        "    x_detects = [int(target_bb[1]) for target_bb in target_bbs]\n",
        "    #get x values for which interpolation has to be performed\n",
        "    x_to_interp = [xframe for xframe in range(len(vid_input)) if xframe not in x_detects]\n",
        "\n",
        "    #get the values for successful detections\n",
        "    x_centers_targets_detec = np.asarray([frame_value[2] for frame_value in target_bbs])\n",
        "    y_centers_targets_detec = np.asarray([frame_value[3] for frame_value in target_bbs])\n",
        "    bbwidths_targets_detec = np.asarray([frame_value[4] for frame_value in target_bbs])\n",
        "    bbheights_targets_detec = np.asarray([frame_value[5] for frame_value in target_bbs])\n",
        "\n",
        "    #interpolate\n",
        "    #numpy.interp(x, xp, fp, left=None, right=None, period=None)\n",
        "    inter_x_centers_target = np.interp(x_to_interp, x_detects, x_centers_targets_detec)\n",
        "    inter_y_centers_target = np.interp(x_to_interp, x_detects, y_centers_targets_detec)\n",
        "    inter_bbwidths_target = np.interp(x_to_interp, x_detects, bbwidths_targets_detec)\n",
        "    inter_bbheights_target = np.interp(x_to_interp, x_detects, bbheights_targets_detec)\n",
        "\n",
        "    #Combine values into single array\n",
        "    #add frame and video indices for new array\n",
        "    #frame indices\n",
        "    all_frames = [frame for frame in range(len(vid_input))]\n",
        "    vid_input[all_frames,1] = range(len(vid_input))\n",
        "    #video indices\n",
        "    vid_input[all_frames,0] = i\n",
        "\n",
        "    #pseudo: vid_input[all_detec_xs][einzeln 2 bis 4 für die werte (in versch zeilen)] =  x_centers_targets_detec bis bbheights_targets_detec in den zeilen\n",
        "    vid_input[x_detects,2] = x_centers_targets_detec\n",
        "    vid_input[x_detects,3] = y_centers_targets_detec\n",
        "    vid_input[x_detects,4] = bbwidths_targets_detec\n",
        "    vid_input[x_detects,5] = bbheights_targets_detec\n",
        "\n",
        "    #und dasselbe für interpolated\n",
        "    vid_input[x_to_interp,2] = inter_x_centers_target\n",
        "    vid_input[x_to_interp,3] = inter_y_centers_target\n",
        "    vid_input[x_to_interp,4] = inter_bbwidths_target\n",
        "    vid_input[x_to_interp,5] = inter_bbheights_target\n",
        "\n",
        "\n",
        "    #Perform interpolation for hands\n",
        "\n",
        "    #get x values for which detection was successful\n",
        "    x_detects = [int(hand_bb[1]) for hand_bb in hand_bbs]\n",
        "    #get x values for which interpolation has to be performed\n",
        "    x_to_interp = [xframe for xframe in range(len(vid_input)) if xframe not in x_detects]\n",
        "\n",
        "    #get the values for successful detections\n",
        "    x_centers_hand_detec = np.asarray([frame_value[2] for frame_value in hand_bbs])\n",
        "    y_centers_hand_detec = np.asarray([frame_value[3] for frame_value in hand_bbs])\n",
        "    bbwidths_hand_detec = np.asarray([frame_value[4] for frame_value in hand_bbs])\n",
        "    bbheights_hand_detec = np.asarray([frame_value[5] for frame_value in hand_bbs])\n",
        "\n",
        "    #interpolate\n",
        "    #numpy.interp(x, xp, fp, left=None, right=None, period=None)\n",
        "    inter_x_centers_hand = np.interp(x_to_interp, x_detects, x_centers_hand_detec)\n",
        "    inter_y_centers_hand = np.interp(x_to_interp, x_detects, y_centers_hand_detec)\n",
        "    inter_bbwidths_hand = np.interp(x_to_interp, x_detects, bbwidths_hand_detec)\n",
        "    inter_bbheights_hand = np.interp(x_to_interp, x_detects, bbheights_hand_detec)\n",
        "\n",
        "    #Combine values into single array\n",
        "    #add frame and video indices for new array\n",
        "    #frame indices\n",
        "    all_frames = [frame for frame in range(len(vid_input))]\n",
        "    vid_input[all_frames,1] = range(len(vid_input))\n",
        "    #video indices\n",
        "    vid_input[all_frames,0] = i\n",
        "\n",
        "    #pseudo: vid_input[all_detec_xs][einzeln 2 bis 4 für die werte (in versch zeilen)] =  x_centers_targets_detec bis bbheights_targets_detec in den zeilen\n",
        "    vid_input[x_detects,6] = x_centers_hand_detec\n",
        "    vid_input[x_detects,7] = y_centers_hand_detec\n",
        "    vid_input[x_detects,8] = bbwidths_hand_detec\n",
        "    vid_input[x_detects,9] = bbheights_hand_detec\n",
        "\n",
        "    #und dasselbe für interpolated\n",
        "    vid_input[x_to_interp,6] = inter_x_centers_hand\n",
        "    vid_input[x_to_interp,7] = inter_y_centers_hand\n",
        "    vid_input[x_to_interp,8] = inter_bbwidths_hand\n",
        "    vid_input[x_to_interp,9] = inter_bbheights_hand\n",
        "\n",
        "\n",
        "    # get first frame with information of target (used for input later)\n",
        "    start_frame = 0\n",
        "    for i, frame in enumerate(vid_input[:, :6]):\n",
        "        if np.isnan(frame).any():\n",
        "            start_frame = i+1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    # 4. Center: Center on the correct bounding boxes of the target fruit\n",
        "    offsets, start_cords = Center(folderpath+video, labelpath+video, vid_input[:, 2:6])\n",
        "\n",
        "    new_target_bbs = vid_input[:,2:6]#.copy()\n",
        "    new_hand_bbs = vid_input[:,6:]#.copy()\n",
        "\n",
        "    #Get offsets of target from center of OG video\n",
        "    x_offsets = offsets[:,0]\n",
        "    y_offsets = offsets[:,1]\n",
        "\n",
        "    start_x = start_cords[1]\n",
        "    start_y = start_cords[0]\n",
        "\n",
        "    #Calculate Bounding boxes in bigger video before centering\n",
        "    #Adjust x\n",
        "    new_target_bbs[:,0] += start_x\n",
        "    new_hand_bbs[:,0] += start_x\n",
        "\n",
        "    #Adjust y\n",
        "    new_target_bbs[:,1] += start_y\n",
        "    new_hand_bbs[:,1] += start_y\n",
        "\n",
        "    #Now adjust for after centering based on offsets\n",
        "    #Adjust x\n",
        "    new_target_bbs[:,0] -= x_offsets\n",
        "    new_hand_bbs[:,0] -= x_offsets\n",
        "\n",
        "    #Adjust y\n",
        "    new_target_bbs[:,1] -= y_offsets\n",
        "    new_hand_bbs[:,1] -= y_offsets\n",
        "\n",
        "\n",
        "    targets_folderID = '1u6jLaX4Mb5UtQzDGtXvQPgwJIbG92YBT'\n",
        "    hands_folderID = '1901iQUf4nCOi1ZPJRuyJeEQ0MweQKlka'\n",
        "\n",
        "    #Create npz files\n",
        "    np.savez('Post_Centering_BBs_Target_Video_{}.npz'.format(i), array=new_target_bbs)\n",
        "    np.savez('Post_Centering_BBs_Hand_Video_{}.npz'.format(i), array=new_hand_bbs)\n",
        "\n",
        "    #Write npz files to drive\n",
        "    Post_Centering_BBs_Target = drive.CreateFile({'title': 'Post_Centering_BBs_Target_Video_{}.npz'.format(i), 'parents': [{'id': targets_folderID}]})\n",
        "    Post_Centering_BBs_Target.SetContentFile('Post_Centering_BBs_Target_Video_{}.npz'.format(i))\n",
        "    Post_Centering_BBs_Target.Upload()\n",
        "\n",
        "    Post_Centering_BBs_Hands = drive.CreateFile({'title': 'Post_Centering_BBs_Hand_Video_{}.npz'.format(i), 'parents': [{'id': hands_folderID}]})\n",
        "    Post_Centering_BBs_Hands.SetContentFile('Post_Centering_BBs_Hand_Video_{}.npz'.format(i))\n",
        "    Post_Centering_BBs_Hands.Upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For Banana video:\n",
        "    #New X Center For Trials2 Banana video should be 1245.589599609375\n",
        "    #New Y Center For Trials2 Banana video should be 587.0914916992188\n",
        "print(new_target_bbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HjTbcr5LIgy",
        "outputId": "d710524c-dbef-4403-d9c7-813740bcfbc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.9      587.91      225.45       167.3]\n",
            " [     1245.4      587.66      213.52       170.9]\n",
            " [     1245.3      587.64      213.42      171.04]\n",
            " [     1245.7      587.53      213.18      171.12]\n",
            " [     1245.1       587.9      217.46      170.94]\n",
            " [     1245.3      587.87      217.73      170.68]\n",
            " [     1245.5      587.47      221.74      171.87]\n",
            " [     1245.4      587.43      221.53       171.8]\n",
            " [     1245.6      587.45      223.55      171.45]\n",
            " [     1245.5      587.33      224.82      170.19]\n",
            " [     1245.6      587.52      224.13      170.36]\n",
            " [     1245.1      587.58      220.95      169.24]\n",
            " [     1245.2      587.54      220.92      169.17]\n",
            " [     1245.6      587.91      217.72      169.05]\n",
            " [     1245.8      587.51      216.62      167.71]\n",
            " [     1245.2      587.95      217.21      170.29]\n",
            " [     1245.1      587.99      217.55      170.43]\n",
            " [     1245.4      587.36       216.8      170.85]\n",
            " [     1245.5      587.94      211.84      169.73]\n",
            " [     1245.3      587.91      211.57      169.66]\n",
            " [     1245.9      587.11      225.25      174.87]\n",
            " [     1245.1      587.94      225.33      175.36]\n",
            " [     1245.1      587.97      225.21       175.4]\n",
            " [     1245.4       587.1      211.88       170.5]\n",
            " [     1245.6      587.55      223.05      173.61]\n",
            " [     1245.3      587.97      206.65      170.12]\n",
            " [     1245.9       587.1      223.93      172.28]\n",
            " [     1245.6      587.31      206.89      169.62]\n",
            " [     1245.5      587.63       212.3       171.2]\n",
            " [     1245.4      587.95      217.72      172.77]\n",
            " [     1245.3      587.26      223.13      174.34]\n",
            " [       1246      587.37      223.64      173.32]\n",
            " [     1245.9      587.53      207.81      169.31]\n",
            " [     1245.9      587.54      207.85      169.28]\n",
            " [     1245.9      587.43      207.31      169.55]\n",
            " [     1245.8      587.43      206.88       169.4]\n",
            " [     1245.7      587.43      206.46      169.26]\n",
            " [     1245.6      587.43      206.03      169.11]\n",
            " [     1245.6      587.21      205.77      170.66]\n",
            " [     1245.6      587.29      206.24       170.3]\n",
            " [     1245.6       587.8      210.15      170.07]\n",
            " [     1245.6      587.32      214.05      169.83]\n",
            " [     1245.6      587.84      217.95       169.6]\n",
            " [     1245.6      587.35      221.85      169.36]\n",
            " [     1245.6      587.87      225.76      169.13]\n",
            " [     1245.6      587.38      229.66       168.9]\n",
            " [     1245.7      587.86      228.56       169.4]\n",
            " [     1245.7      587.26       228.8      169.72]\n",
            " [     1245.7      587.31       228.8      169.72]\n",
            " [     1245.8      587.33      229.88      170.41]\n",
            " [     1245.1      587.55      232.32      166.26]\n",
            " [     1245.2      587.07      234.09      163.84]\n",
            " [     1245.3      587.09      234.19      163.74]\n",
            " [     1245.8      587.85      234.12      163.39]\n",
            " [     1245.2      587.61      234.04      163.05]\n",
            " [     1245.7      587.37      233.96       162.7]\n",
            " [     1245.2      587.13      233.88      162.36]\n",
            " [     1245.7      587.89       233.8      162.01]\n",
            " [     1245.2      587.65      233.72      161.67]\n",
            " [     1245.6      587.41      233.64      161.32]\n",
            " [     1245.1      587.17      233.56      160.98]\n",
            " [     1245.6      587.93      233.49      160.63]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]\n",
            " [     1245.1      587.69      233.41      160.28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(new_target_bbs)\n",
        "#print(\"\\n\",vid_input[:, 2:6])\n",
        "#print(\"\\n\",offsets)\n",
        "if(new_target_bbs == vid_input[:,2:6]).all():\n",
        "    print(\"error\")\n",
        "\n",
        "print(x_offsets)\n",
        "#print(y_offsets)\n",
        "\"\"\"\n",
        "test_bbs_target = vid_input[:,2:6]\n",
        "test_bbs_target_x = test_bbs_target[:,0]\n",
        "test_bbs_target_x_new = test_bbs_target[:,0] + x_offsets\n",
        "#print(test_bbs_target_new)\n",
        "print(\"test_bbs_target_x:\",test_bbs_target_x)\n",
        "print(\"test_bbs_target_new:\",test_bbs_target_x_new)\n",
        "if(test_bbs_target_x == test_bbs_target_x_new).all():\n",
        "    print(\"error\")\n",
        "\n",
        "#Adjust x\n",
        "test_bbs_target[:,0] += x_offsets\n",
        "print(test_bbs_target)\n",
        "print(\"\\n\",new_target_bbs)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XBVyInHa4MNr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "ac615a8f-8538-499d-cd1f-f9e3a0f292e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error\n",
            "[        284         284         284         284         284         284         284         284         284         284         284         284         284         284         284         269         269         269         268         268         265         265         265         265         266         268\n",
            "         268         269         270         271         271         271         273         273         280         280         280         272         281         274         282         274         277         280         283         282         274         274         274         274         274         274\n",
            "         273         273         275         277         279         281         283         285         284         284         284         281         277         273         273         270         268         265         263         260         258         255         253         250         248         248\n",
            "         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248         248\n",
            "         248         248         248         248         248         248         248         248         248         248         248         248]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_bbs_target = vid_input[:,2:6]\\ntest_bbs_target_x = test_bbs_target[:,0]\\ntest_bbs_target_x_new = test_bbs_target[:,0] + x_offsets\\n#print(test_bbs_target_new)\\nprint(\"test_bbs_target_x:\",test_bbs_target_x)\\nprint(\"test_bbs_target_new:\",test_bbs_target_x_new)\\nif(test_bbs_target_x == test_bbs_target_x_new).all():\\n    print(\"error\")\\n\\n#Adjust x\\ntest_bbs_target[:,0] += x_offsets\\nprint(test_bbs_target)\\nprint(\"\\n\",new_target_bbs)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}