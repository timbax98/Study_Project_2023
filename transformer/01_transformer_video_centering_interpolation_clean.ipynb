{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDqBMS1KBMo2"
      },
      "source": [
        "# **Study Project:** *Transformer model for prediction of grasping movements*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC1eRfTGuWyA",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0yyZ3iVmk2a",
        "outputId": "0e51d2cf-eb22-4c29-9023-ab33ad5986c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.199-py3-none-any.whl (644 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.5/644.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (17.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7Sn6IstgaA6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "from scipy import interpolate\n",
        "#!pip install ultralytics\n",
        "#ipd.clear_output()\n",
        "import ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Debugging\n",
        "import sys"
      ],
      "metadata": {
        "id": "-nfl24K63ZMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJf8BcE-to4F",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCgIq5tJPEHX"
      },
      "outputs": [],
      "source": [
        "def GetBoundingBoxes(videopath):\n",
        "    \"\"\"\n",
        "    Extract bounding boxes from video.\n",
        "\n",
        "    Args:\n",
        "    videopath -- path to video to extract bounding boxes from.\n",
        "    \"\"\"\n",
        "    # Use object tracker to get the bounding boxes and classIDs in a 'results' object\n",
        "    bbs = np.zeros((1, 7))\n",
        "    # It is also possible to pass the whole folder as path,\n",
        "    # but we still want the flexibility to access single videos\n",
        "    results = model.track(source=videopath, tracker=\"bytetrack.yaml\")\n",
        "\n",
        "    # Get class names\n",
        "    classes = results[0].names\n",
        "\n",
        "    # Iterate through each frame of a video to get all bounding boxes for a frame\n",
        "    for frame in range(len(results)):\n",
        "\n",
        "        # x_center, y_center, bbwidth, bbheight of bbs of this frame\n",
        "        xywh = results[frame].boxes.xywh.detach().cpu().numpy()\n",
        "        n = len(xywh) # number of bounding boxes\n",
        "        cls = results[frame].boxes.cls.detach().cpu().numpy().reshape((n,1))\n",
        "        # if the object tracker is currently tracking at least one object, save the trackingID for that object, else fill with -1 placeholder\n",
        "        trackingID = results[frame].boxes.id.detach().cpu().numpy().reshape((n,1)) if results[frame].boxes.is_track else np.repeat(-1, n).reshape((n,1))\n",
        "        frame_count = np.repeat(frame, n).reshape((n,1))\n",
        "\n",
        "        # bind the data together for one frame\n",
        "        data = np.concatenate((frame_count, cls, xywh, trackingID), axis=1)\n",
        "        # add all data of this frame to all data of this video\n",
        "        bbs = np.concatenate((bbs, data), axis=0)\n",
        "\n",
        "    return bbs, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjTnqYuouFnu"
      },
      "outputs": [],
      "source": [
        "def Center(videopath, to_path, bounding_boxes, width=1920, height=1120):\n",
        "\n",
        "    \"\"\"\n",
        "    Create a video where a target object is always centered.\n",
        "\n",
        "    Args:\n",
        "    videopath -- path to video to center\n",
        "    to_path -- location to export centered video to\n",
        "    bounding_boxes -- bbs of object to center on\n",
        "    width -- output video dim x, default=1920\n",
        "    height -- output video dim y, default=1120\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Read video\n",
        "    cap = cv2.VideoCapture(videopath)\n",
        "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    video = np.zeros((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
        "    fc = 0\n",
        "    ret = True\n",
        "\n",
        "    while (fc < frameCount and ret):\n",
        "        ret, video[fc] = cap.read()\n",
        "        fc += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "    # 2. Correct video\n",
        "    bbs = bounding_boxes\n",
        "    width = width\n",
        "    height = height\n",
        "    center_x = (width/2)\n",
        "    center_y = (height/2)\n",
        "    x_dists = []\n",
        "    y_dists = []\n",
        "\n",
        "    for box_pos in bbs:\n",
        "        if np.isnan(box_pos).any():\n",
        "            # if all values were nan, then 0 would be the max, so the corrected video would be the original video\n",
        "            x_dists.append(0)\n",
        "            y_dists.append(0)\n",
        "        else:\n",
        "            # get center of bb\n",
        "            #center_x_bb = (box_pos[0] + box_pos[2]) / 2\n",
        "            #center_y_bb = (box_pos[1] + box_pos[3]) / 2\n",
        "            center_x_bb = box_pos[0]\n",
        "            center_y_bb = box_pos[1]\n",
        "            # calculate distances\n",
        "            x_dists.append(abs(center_x-center_x_bb))\n",
        "            y_dists.append(abs(center_y-center_y_bb))\n",
        "\n",
        "\n",
        "    video_corrected = np.zeros((len(video),\n",
        "                              int(height + max(y_dists)*2),\n",
        "                              int(width + max(x_dists)*2),\n",
        "                              3), dtype=int)\n",
        "\n",
        "\n",
        "    # 3. Center video\n",
        "    # get coords for placement height and width. may be switched\n",
        "    start_row = (video_corrected.shape[1] - height) // 2\n",
        "    start_col = (video_corrected.shape[2] - width) // 2\n",
        "\n",
        "    for idx, frame in enumerate(video):\n",
        "\n",
        "        # get matching bb\n",
        "        box_curr = bbs[idx]\n",
        "\n",
        "        # If there is no information on fruit, color the frame black (skip iter)\n",
        "        if np.isnan(box_curr).any():\n",
        "            continue\n",
        "            # frame[:] = 0\n",
        "            # frame = np.zeros(frame.shape)\n",
        "        else:\n",
        "            # get center of bb\n",
        "            #center_x_bb = (box_curr[0] + box_curr[2]) / 2\n",
        "            #center_y_bb = (box_curr[1] + box_curr[3]) / 2\n",
        "            center_x_bb = box_curr[0]\n",
        "            center_y_bb = box_curr[1]\n",
        "\n",
        "            # get offset of center\n",
        "            # pos if bounding box is to right of center, else negative\n",
        "            x_offset = int(center_x_bb - center_x)\n",
        "            # pos if bounding box is below center, else negative\n",
        "            y_offset = int(center_y_bb - center_y)\n",
        "\n",
        "            #!coordinates until here are for old video\n",
        "\n",
        "            #get fitting indices for new video\n",
        "            fixed_start_row = start_row - y_offset\n",
        "            fixed_start_col = start_col - x_offset\n",
        "\n",
        "            fixed_end_row = fixed_start_row + height\n",
        "            fixed_end_col = fixed_start_col + width\n",
        "\n",
        "            # Checkup\n",
        "            if((fixed_start_row or\n",
        "                fixed_start_col or\n",
        "                fixed_end_row or\n",
        "                fixed_end_col) < 0):\n",
        "                print(\"Negative Index!\")\n",
        "\n",
        "            # Save centered + corrected in new video\n",
        "            video_corrected[idx][fixed_start_row:fixed_end_row,\n",
        "                              fixed_start_col:fixed_end_col] = frame\n",
        "\n",
        "\n",
        "    # 4. Save video\n",
        "    video_corrected = np.uint8(video_corrected)\n",
        "\n",
        "    height_new = int(video_corrected.shape[1])\n",
        "    width_new = int(video_corrected.shape[2])\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(to_path[:-4]+'_centered.mp4', fourcc, FPS, (width_new, height_new), True)\n",
        "    for idx in range(len(video)):\n",
        "        out.write(video_corrected[idx])\n",
        "    out.release()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1OVMgLRiBSf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Video centering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gJT7yT2FnWFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxE-wPP7SFe3"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = ultralytics.YOLO('yolov8n.pt')\n",
        "\n",
        "# List videos\n",
        "#folderpath = '/content/drive/MyDrive/Study_Project_Grasping_Copy/Transformer/Trials/'\n",
        "#labelpath = '/content/drive/MyDrive/Study_Project_Grasping_Copy/Transformer/Labels/'\n",
        "#folder = [f for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f))]\n",
        "folderpath = '/content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/'\n",
        "labelpath = '/content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Labels2/'\n",
        "folder = [f for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f))]\n",
        "\n",
        "# Example video\n",
        "#ipd.Video(folderpath+'banana02.mp4', width=1920/1.7, height=1120/1.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkb5TAAFGKWp"
      },
      "source": [
        "**Procedure:**\n",
        "1. Get the bounding boxes of all objects in each raw video.\n",
        "2. Filter for hand and target object bounding boxes.\n",
        "3. Target centering: fixate the target object (TARGET.mp4) using interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiJg0H1RXt_v",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Iterate through every video file in the folder\n",
        "for i, video in enumerate(folder):\n",
        "\n",
        "    print(f\"Video {i+1}/{len(folder)}: {video}\\n\")\n",
        "\n",
        "    # Get length of input array (number of frames)\n",
        "    cap = cv2.VideoCapture(folderpath+video)\n",
        "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "    # 1. Get all bbs of this video\n",
        "    # (x_center, y_center, bbwidth, bbheight, class, trackerID, frame, filename)\n",
        "    bbs, classes = GetBoundingBoxes(folderpath+video)\n",
        "    video_count = np.repeat(i, len(bbs)).reshape((len(bbs),1))\n",
        "    bbs = np.concatenate((video_count, bbs), axis=1)\n",
        "\n",
        "\n",
        "    # 2. Filter for hand and target fruit by class\n",
        "    # Get the target class to filter for as number\n",
        "    if '.mov' in video:\n",
        "        # returns the string name without '00.mp4' ending (could add try-catch)\n",
        "        target_class = video[:-6]\n",
        "    #added this, why was it .mov??\n",
        "    elif '.mp4' in video:\n",
        "        # returns the string name without '00.mp4' ending (could add try-catch)\n",
        "        target_class = video[:-6]\n",
        "\n",
        "\n",
        "    # Get class number from lookup table\n",
        "    target_class = list(classes.keys())[list(classes.values()).index(target_class)]\n",
        "    hand_class = list(classes.keys())[list(classes.values()).index('person')]\n",
        "\n",
        "    # Filter data\n",
        "    target_bbs = bbs[(bbs[:, 2] == target_class)]\n",
        "    hand_bbs = bbs[(bbs[:, 2] == hand_class)]\n",
        "\n",
        "    # To Do: Use trackerID to track the same object\n",
        "    # -> could be added later on, as we only use one object per trial?\n",
        "    # -> but if we have detection of more than one target fruit, it breaks something\n",
        "    # because it will try to calculate positions from more than one bb per frame.\n",
        "    # problem so far: if detection ends for one frame, a new trackerID is assigned\n",
        "    # -> for this approach to work we would have to compare the position of the\n",
        "    # last tracked trackerID to each object with newly assigned trackerID and\n",
        "    # take the closest one, and interpolate position if there are no new trackerIDs detected.\n",
        "    # for now, we leave it out: remove class and trackingID\n",
        "    target_bbs = target_bbs[:, [0,1,3,4,5,6]]\n",
        "    hand_bbs = hand_bbs[:, [0,1,3,4,5,6]]\n",
        "\n",
        "    # 3. Interpolation: take last known bounding box positions\n",
        "    # (x_center_fruit, y_center_fruit, bbwidth_fruit, bbheight_fruit, class, trackerID, frame, filename, x_center_hand, y_center_hand, bbwidth_hand, bbheight_hand)\n",
        "    vid_input = np.zeros((frameCount, 10))\n",
        "    vid_input[:] = np.nan # use as 'no information' instead of nan, because nan is a string\n",
        "\n",
        "    #Perform interpolation for targets\n",
        "\n",
        "    #get x values for which detection was successful\n",
        "    x_detects = [int(target_bb[1]) for target_bb in target_bbs]\n",
        "    #get x values for which interpolation has to be performed\n",
        "    x_to_interp = [xframe for xframe in range(len(vid_input)) if xframe not in x_detects]\n",
        "\n",
        "    #get the values for successful detections\n",
        "    x_centers_targets_detec = np.asarray([frame_value[2] for frame_value in target_bbs])\n",
        "    y_centers_targets_detec = np.asarray([frame_value[3] for frame_value in target_bbs])\n",
        "    bbwidths_targets_detec = np.asarray([frame_value[4] for frame_value in target_bbs])\n",
        "    bbheights_targets_detec = np.asarray([frame_value[5] for frame_value in target_bbs])\n",
        "\n",
        "    #interpolate\n",
        "    #numpy.interp(x, xp, fp, left=None, right=None, period=None)\n",
        "    inter_x_centers_target = np.interp(x_to_interp, x_detects, x_centers_targets_detec)\n",
        "    inter_y_centers_target = np.interp(x_to_interp, x_detects, y_centers_targets_detec)\n",
        "    inter_bbwidths_target = np.interp(x_to_interp, x_detects, bbwidths_targets_detec)\n",
        "    inter_bbheights_target = np.interp(x_to_interp, x_detects, bbheights_targets_detec)\n",
        "\n",
        "    #Combine values into single array\n",
        "    #add frame and video indices for new array\n",
        "    #frame indices\n",
        "    all_frames = [frame for frame in range(len(vid_input))]\n",
        "    vid_input[all_frames,1] = range(len(vid_input))\n",
        "    #video indices\n",
        "    vid_input[all_frames,0] = i\n",
        "\n",
        "    #pseudo: vid_input[all_detec_xs][einzeln 2 bis 4 für die werte (in versch zeilen)] =  x_centers_targets_detec bis bbheights_targets_detec in den zeilen\n",
        "    vid_input[x_detects,2] = x_centers_targets_detec\n",
        "    vid_input[x_detects,3] = y_centers_targets_detec\n",
        "    vid_input[x_detects,4] = bbwidths_targets_detec\n",
        "    vid_input[x_detects,5] = bbheights_targets_detec\n",
        "\n",
        "    #und dasselbe für interpolated\n",
        "    vid_input[x_to_interp,2] = inter_x_centers_target\n",
        "    vid_input[x_to_interp,3] = inter_y_centers_target\n",
        "    vid_input[x_to_interp,4] = inter_bbwidths_target\n",
        "    vid_input[x_to_interp,5] = inter_bbheights_target\n",
        "\n",
        "    # get first frame with information of target (used for input later)\n",
        "    start_frame = 0\n",
        "    for i, frame in enumerate(vid_input[:, :6]):\n",
        "        if np.isnan(frame).any():\n",
        "            start_frame = i+1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    # 4. Center: Center on the correct bounding boxes of the target fruit\n",
        "    Center(folderpath+video, labelpath+video, vid_input[:, 2:6])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}