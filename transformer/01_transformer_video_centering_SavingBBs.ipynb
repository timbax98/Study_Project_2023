{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDqBMS1KBMo2"
      },
      "source": [
        "# **Study Project:** *Transformer model for prediction of grasping movements*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC1eRfTGuWyA",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install pydrive\n",
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0yyZ3iVmk2a",
        "outputId": "13fd9807-0bda-4619-d531-794c85e7d1fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.202-py3-none-any.whl (644 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/644.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/644.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.202\n",
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.10/dist-packages (from pydrive) (2.84.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (6.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (4.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.2->pydrive) (5.3.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->pydrive) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F7Sn6IstgaA6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "\n",
        "from scipy import interpolate\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#!pip install ultralytics\n",
        "#ipd.clear_output()\n",
        "import ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Debugging\n",
        "import sys"
      ],
      "metadata": {
        "id": "-nfl24K63ZMC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJf8BcE-to4F",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LCgIq5tJPEHX"
      },
      "outputs": [],
      "source": [
        "def GetBoundingBoxes(videopath):\n",
        "    \"\"\"\n",
        "    Extract bounding boxes from video.\n",
        "\n",
        "    Args:\n",
        "    videopath -- path to video to extract bounding boxes from.\n",
        "    \"\"\"\n",
        "    # Use object tracker to get the bounding boxes and classIDs in a 'results' object\n",
        "    bbs = np.zeros((1, 7))\n",
        "    # It is also possible to pass the whole folder as path,\n",
        "    # but we still want the flexibility to access single videos\n",
        "    results = model.track(source=videopath, tracker=\"bytetrack.yaml\")\n",
        "\n",
        "    # Get class names\n",
        "    classes = results[0].names\n",
        "\n",
        "    # Iterate through each frame of a video to get all bounding boxes for a frame\n",
        "    for frame in range(len(results)):\n",
        "\n",
        "        # x_center, y_center, bbwidth, bbheight of bbs of this frame\n",
        "        xywh = results[frame].boxes.xywh.detach().cpu().numpy()\n",
        "        n = len(xywh) # number of bounding boxes\n",
        "        cls = results[frame].boxes.cls.detach().cpu().numpy().reshape((n,1))\n",
        "        # if the object tracker is currently tracking at least one object, save the trackingID for that object, else fill with -1 placeholder\n",
        "        trackingID = results[frame].boxes.id.detach().cpu().numpy().reshape((n,1)) if results[frame].boxes.is_track else np.repeat(-1, n).reshape((n,1))\n",
        "        frame_count = np.repeat(frame, n).reshape((n,1))\n",
        "\n",
        "        # bind the data together for one frame\n",
        "        data = np.concatenate((frame_count, cls, xywh, trackingID), axis=1)\n",
        "        # add all data of this frame to all data of this video\n",
        "        bbs = np.concatenate((bbs, data), axis=0)\n",
        "\n",
        "    return bbs, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KjTnqYuouFnu"
      },
      "outputs": [],
      "source": [
        "def Center(videopath, to_path, bounding_boxes, width=1920, height=1120):\n",
        "\n",
        "    \"\"\"\n",
        "    Create a video where a target object is always centered.\n",
        "\n",
        "    Args:\n",
        "    videopath -- path to video to center\n",
        "    to_path -- location to export centered video to\n",
        "    bounding_boxes -- bbs of object to center on\n",
        "    width -- output video dim x, default=1920\n",
        "    height -- output video dim y, default=1120\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Read video\n",
        "    cap = cv2.VideoCapture(videopath)\n",
        "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    video = np.zeros((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
        "    fc = 0\n",
        "    ret = True\n",
        "\n",
        "    while (fc < frameCount and ret):\n",
        "        ret, video[fc] = cap.read()\n",
        "        fc += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "    # 2. Correct video\n",
        "    bbs = bounding_boxes\n",
        "    width = width\n",
        "    height = height\n",
        "    center_x = (width/2)\n",
        "    center_y = (height/2)\n",
        "    x_dists = []\n",
        "    y_dists = []\n",
        "\n",
        "    for box_pos in bbs:\n",
        "        if np.isnan(box_pos).any():\n",
        "            # if all values were nan, then 0 would be the max, so the corrected video would be the original video\n",
        "            x_dists.append(0)\n",
        "            y_dists.append(0)\n",
        "        else:\n",
        "            # get center of bb\n",
        "            #center_x_bb = (box_pos[0] + box_pos[2]) / 2\n",
        "            #center_y_bb = (box_pos[1] + box_pos[3]) / 2\n",
        "            center_x_bb = box_pos[0]\n",
        "            center_y_bb = box_pos[1]\n",
        "            # calculate distances\n",
        "            x_dists.append(abs(center_x-center_x_bb))\n",
        "            y_dists.append(abs(center_y-center_y_bb))\n",
        "\n",
        "\n",
        "    video_corrected = np.zeros((len(video),\n",
        "                              int(height + max(y_dists)*2),\n",
        "                              int(width + max(x_dists)*2),\n",
        "                              3), dtype=int)\n",
        "\n",
        "\n",
        "    # 3. Center video\n",
        "    # get coords for placement height and width. may be switched\n",
        "    start_row = (video_corrected.shape[1] - height) // 2\n",
        "    start_col = (video_corrected.shape[2] - width) // 2\n",
        "\n",
        "    offsets = np.zeros(shape=(len(video),2))\n",
        "\n",
        "    for idx, frame in enumerate(video):\n",
        "\n",
        "        # get matching bb\n",
        "        box_curr = bbs[idx]\n",
        "\n",
        "        # If there is no information on fruit, color the frame black (skip iter)\n",
        "        if np.isnan(box_curr).any():\n",
        "            continue\n",
        "            # frame[:] = 0\n",
        "            # frame = np.zeros(frame.shape)\n",
        "        else:\n",
        "            # get center of bb\n",
        "            #center_x_bb = (box_curr[0] + box_curr[2]) / 2\n",
        "            #center_y_bb = (box_curr[1] + box_curr[3]) / 2\n",
        "            center_x_bb = box_curr[0]\n",
        "            center_y_bb = box_curr[1]\n",
        "\n",
        "            # get offset of center\n",
        "            # pos if bounding box is to right of center, else negative\n",
        "            x_offset = int(center_x_bb - center_x)\n",
        "            # pos if bounding box is below center, else negative\n",
        "            y_offset = int(center_y_bb - center_y)\n",
        "\n",
        "            #!coordinates until here are for old video\n",
        "\n",
        "            #get fitting indices for new video\n",
        "            fixed_start_row = start_row - y_offset\n",
        "            fixed_start_col = start_col - x_offset\n",
        "\n",
        "            fixed_end_row = fixed_start_row + height\n",
        "            fixed_end_col = fixed_start_col + width\n",
        "\n",
        "            # Checkup\n",
        "            if((fixed_start_row or\n",
        "                fixed_start_col or\n",
        "                fixed_end_row or\n",
        "                fixed_end_col) < 0):\n",
        "                print(\"Negative Index!\")\n",
        "\n",
        "            # Save centered + corrected in new video\n",
        "            video_corrected[idx][fixed_start_row:fixed_end_row,\n",
        "                              fixed_start_col:fixed_end_col] = frame\n",
        "\n",
        "            #Save offsets\n",
        "            offsets[idx,:] = x_offset, y_offset\n",
        "\n",
        "\n",
        "    # 4. Save video\n",
        "    video_corrected = np.uint8(video_corrected)\n",
        "\n",
        "    height_new = int(video_corrected.shape[1])\n",
        "    width_new = int(video_corrected.shape[2])\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(to_path[:-4]+'_centered.mp4', fourcc, FPS, (width_new, height_new), True)\n",
        "    for idx in range(len(video)):\n",
        "        out.write(video_corrected[idx])\n",
        "    out.release()\n",
        "\n",
        "    return offsets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1OVMgLRiBSf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Video centering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gJT7yT2FnWFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb32b16-c1c6-4c5b-f25d-b6ca42db7c0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Acess google drive with pydrive to safe BBs and Offsets\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Set the destination Google Drive folder ID. Folder Name: Centering_OffsetsAndBBs under Data\n",
        "offsets_folder_id = '1d6hcNnHVwsEexY2GDoHGCSLQZYZUbfcs'\n",
        "bbs_targets_folder_id = '1yS_eBVOe-f5R-UIsyyZiWiG37t0s5od2'\n",
        "bbs_hands_folder_id = '1mVdxyoVfBp6LU-KNgWwZ28NLDEn9KNf0'"
      ],
      "metadata": {
        "id": "KQIFJ6oGhxFR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FxE-wPP7SFe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6697f0e7-16a3-402f-f798-8f4dfc828a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 69.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "model = ultralytics.YOLO('yolov8n.pt')\n",
        "\n",
        "# List videos\n",
        "#folderpath = '/content/drive/MyDrive/Study_Project_Grasping_Copy/Transformer/Trials/'\n",
        "#labelpath = '/content/drive/MyDrive/Study_Project_Grasping_Copy/Transformer/Labels/'\n",
        "#folder = [f for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f))]\n",
        "folderpath = '/content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/'\n",
        "labelpath = '/content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Labels2/'\n",
        "folder = [f for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f))]\n",
        "\n",
        "# Example video\n",
        "#ipd.Video(folderpath+'banana02.mp4', width=1920/1.7, height=1120/1.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkb5TAAFGKWp"
      },
      "source": [
        "**Procedure:**\n",
        "1. Get the bounding boxes of all objects in each raw video.\n",
        "2. Filter for hand and target object bounding boxes.\n",
        "3. Target centering: fixate the target object (TARGET.mp4) using interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fiJg0H1RXt_v",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163d538e-ff3b-431e-9d9e-187ac0255e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video 1/1: banana02.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lapx>=0.5.2'] not found, attempting AutoUpdate...\n",
            "Collecting lapx>=0.5.2\n",
            "  Downloading lapx-0.5.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 10.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: Cython>=0.29.32 in /usr/local/lib/python3.10/dist-packages (from lapx>=0.5.2) (3.0.4)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lapx>=0.5.2) (1.23.5)\n",
            "Installing collected packages: lapx\n",
            "Successfully installed lapx-0.5.5\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 7.2s, installed 1 package: ['lapx>=0.5.2']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (1/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 bed, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 194.1ms\n",
            "video 1/1 (2/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 bed, 1 tv, 1 laptop, 1 keyboard, 8.9ms\n",
            "video 1/1 (3/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 bed, 1 tv, 1 laptop, 1 keyboard, 14.5ms\n",
            "video 1/1 (4/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 keyboard, 8.8ms\n",
            "video 1/1 (5/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 keyboard, 8.5ms\n",
            "video 1/1 (6/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 keyboard, 13.7ms\n",
            "video 1/1 (7/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 laptop, 1 keyboard, 2 cell phones, 16.2ms\n",
            "video 1/1 (8/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 tv, 1 laptop, 1 keyboard, 2 cell phones, 1 book, 7.9ms\n",
            "video 1/1 (9/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 tv, 1 laptop, 1 keyboard, 2 cell phones, 1 book, 16.7ms\n",
            "video 1/1 (10/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 2 cell phones, 2 books, 19.2ms\n",
            "video 1/1 (11/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 15.7ms\n",
            "video 1/1 (12/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 2 books, 15.1ms\n",
            "video 1/1 (13/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 1 microwave, 8.0ms\n",
            "video 1/1 (14/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 1 microwave, 14.6ms\n",
            "video 1/1 (15/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 keyboard, 2 cell phones, 1 book, 9.7ms\n",
            "video 1/1 (16/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 tv, 2 laptops, 1 keyboard, 1 cell phone, 9.8ms\n",
            "video 1/1 (17/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 tv, 2 laptops, 1 keyboard, 1 cell phone, 15.1ms\n",
            "video 1/1 (18/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 tvs, 2 laptops, 2 cell phones, 18.5ms\n",
            "video 1/1 (19/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 13.4ms\n",
            "video 1/1 (20/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 14.2ms\n",
            "video 1/1 (21/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 17.3ms\n",
            "video 1/1 (22/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 14.4ms\n",
            "video 1/1 (23/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 orange, 1 keyboard, 1 cell phone, 1 book, 13.1ms\n",
            "video 1/1 (24/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 14.9ms\n",
            "video 1/1 (25/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 1 keyboard, 1 cell phone, 8.1ms\n",
            "video 1/1 (26/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 apple, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 9.7ms\n",
            "video 1/1 (27/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 banana, 1 apple, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 9.8ms\n",
            "video 1/1 (28/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 9.8ms\n",
            "video 1/1 (29/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 19.9ms\n",
            "video 1/1 (30/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 12.9ms\n",
            "video 1/1 (31/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 27.2ms\n",
            "video 1/1 (32/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 15.4ms\n",
            "video 1/1 (33/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 cell phone, 13.7ms\n",
            "video 1/1 (34/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 cell phone, 15.4ms\n",
            "video 1/1 (35/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 2 tvs, 1 cell phone, 15.3ms\n",
            "video 1/1 (36/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 laptop, 1 cell phone, 13.2ms\n",
            "video 1/1 (37/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 laptop, 1 cell phone, 13.5ms\n",
            "video 1/1 (38/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 laptop, 1 cell phone, 14.9ms\n",
            "video 1/1 (39/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 14.6ms\n",
            "video 1/1 (40/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 14.7ms\n",
            "video 1/1 (41/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 8.2ms\n",
            "video 1/1 (42/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 8.5ms\n",
            "video 1/1 (43/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 8.1ms\n",
            "video 1/1 (44/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 cell phone, 17.0ms\n",
            "video 1/1 (45/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 13.8ms\n",
            "video 1/1 (46/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 11.6ms\n",
            "video 1/1 (47/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 7.9ms\n",
            "video 1/1 (48/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 9.5ms\n",
            "video 1/1 (49/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 26.3ms\n",
            "video 1/1 (50/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 orange, 1 tv, 1 laptop, 1 cell phone, 12.0ms\n",
            "video 1/1 (51/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 cell phone, 8.3ms\n",
            "video 1/1 (52/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 cell phone, 16.4ms\n",
            "video 1/1 (53/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 keyboard, 1 cell phone, 1 book, 8.5ms\n",
            "video 1/1 (54/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 2 tvs, 1 keyboard, 1 cell phone, 21.3ms\n",
            "video 1/1 (55/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 2 tvs, 1 laptop, 1 keyboard, 1 cell phone, 8.5ms\n",
            "video 1/1 (56/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 2 tvs, 1 laptop, 1 keyboard, 1 cell phone, 15.3ms\n",
            "video 1/1 (57/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 8.3ms\n",
            "video 1/1 (58/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 1 book, 8.1ms\n",
            "video 1/1 (59/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 apple, 1 orange, 1 tv, 1 laptop, 1 keyboard, 1 cell phone, 1 book, 8.4ms\n",
            "video 1/1 (60/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 keyboard, 1 cell phone, 1 book, 9.2ms\n",
            "video 1/1 (61/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 tv, 2 laptops, 1 cell phone, 25.3ms\n",
            "video 1/1 (62/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 laptop, 1 cell phone, 1 book, 16.9ms\n",
            "video 1/1 (63/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 orange, 1 tv, 1 laptop, 1 cell phone, 13.5ms\n",
            "video 1/1 (64/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 banana, 1 apple, 1 orange, 1 laptop, 1 cell phone, 1 book, 16.0ms\n",
            "video 1/1 (65/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 laptops, 1 cell phone, 24.6ms\n",
            "video 1/1 (66/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 15.0ms\n",
            "video 1/1 (67/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 banana, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 17.0ms\n",
            "video 1/1 (68/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 2 books, 22.2ms\n",
            "video 1/1 (69/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 1 keyboard, 1 cell phone, 2 books, 15.9ms\n",
            "video 1/1 (70/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 2 laptops, 1 cell phone, 1 book, 14.3ms\n",
            "video 1/1 (71/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 apple, 1 orange, 2 laptops, 1 cell phone, 1 book, 12.9ms\n",
            "video 1/1 (72/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 2 laptops, 1 cell phone, 1 book, 24.2ms\n",
            "video 1/1 (73/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 2 persons, 1 orange, 1 laptop, 1 keyboard, 1 cell phone, 1 book, 13.7ms\n",
            "video 1/1 (74/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 14.6ms\n",
            "video 1/1 (75/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 17.9ms\n",
            "video 1/1 (76/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 15.2ms\n",
            "video 1/1 (77/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 14.4ms\n",
            "video 1/1 (78/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 18.5ms\n",
            "video 1/1 (79/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 17.9ms\n",
            "video 1/1 (80/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 13.2ms\n",
            "video 1/1 (81/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 cell phone, 14.0ms\n",
            "video 1/1 (82/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 8.6ms\n",
            "video 1/1 (83/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 12.8ms\n",
            "video 1/1 (84/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 cell phone, 12.5ms\n",
            "video 1/1 (85/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 25.2ms\n",
            "video 1/1 (86/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 9.8ms\n",
            "video 1/1 (87/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 15.3ms\n",
            "video 1/1 (88/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 14.9ms\n",
            "video 1/1 (89/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 9.0ms\n",
            "video 1/1 (90/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 27.7ms\n",
            "video 1/1 (91/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 13.4ms\n",
            "video 1/1 (92/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 19.1ms\n",
            "video 1/1 (93/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 8.4ms\n",
            "video 1/1 (94/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 14.1ms\n",
            "video 1/1 (95/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 17.6ms\n",
            "video 1/1 (96/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 14.9ms\n",
            "video 1/1 (97/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 18.5ms\n",
            "video 1/1 (98/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 19.3ms\n",
            "video 1/1 (99/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 15.1ms\n",
            "video 1/1 (100/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 18.0ms\n",
            "video 1/1 (101/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 14.9ms\n",
            "video 1/1 (102/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 20.4ms\n",
            "video 1/1 (103/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 15.3ms\n",
            "video 1/1 (104/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 keyboard, 1 cell phone, 15.4ms\n",
            "video 1/1 (105/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 14.1ms\n",
            "video 1/1 (106/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 20.8ms\n",
            "video 1/1 (107/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 13.7ms\n",
            "video 1/1 (108/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 14.6ms\n",
            "video 1/1 (109/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 16.7ms\n",
            "video 1/1 (110/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 12.2ms\n",
            "video 1/1 (111/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 14.8ms\n",
            "video 1/1 (112/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 11.3ms\n",
            "video 1/1 (113/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 18.2ms\n",
            "video 1/1 (114/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 13.6ms\n",
            "video 1/1 (115/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 24.9ms\n",
            "video 1/1 (116/116) /content/drive/MyDrive/Study Project: Grasping/Transformer/Data/Trials2/banana02.mp4: 384x640 1 person, 1 orange, 1 keyboard, 1 cell phone, 17.7ms\n",
            "Speed: 7.8ms preprocess, 16.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "# Iterate through every video file in the folder\n",
        "for i, video in enumerate(folder):\n",
        "\n",
        "    print(f\"Video {i+1}/{len(folder)}: {video}\\n\")\n",
        "\n",
        "    # Get length of input array (number of frames)\n",
        "    cap = cv2.VideoCapture(folderpath+video)\n",
        "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "    # 1. Get all bbs of this video\n",
        "    # (x_center, y_center, bbwidth, bbheight, class, trackerID, frame, filename)\n",
        "    bbs, classes = GetBoundingBoxes(folderpath+video)\n",
        "    video_count = np.repeat(i, len(bbs)).reshape((len(bbs),1))\n",
        "    bbs = np.concatenate((video_count, bbs), axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Filter for hand and target fruit by class\n",
        "    # Get the target class to filter for as number\n",
        "    if '.mov' in video:\n",
        "        # returns the string name without '00.mp4' ending (could add try-catch)\n",
        "        target_class = video[:-6]\n",
        "    #added this, why was it .mov??\n",
        "    elif '.mp4' in video:\n",
        "        # returns the string name without '00.mp4' ending (could add try-catch)\n",
        "        target_class = video[:-6]\n",
        "\n",
        "\n",
        "    # Get class number from lookup table\n",
        "    target_class = list(classes.keys())[list(classes.values()).index(target_class)]\n",
        "    hand_class = list(classes.keys())[list(classes.values()).index('person')]\n",
        "\n",
        "    # Filter data\n",
        "    target_bbs = bbs[(bbs[:, 2] == target_class)]\n",
        "    hand_bbs = bbs[(bbs[:, 2] == hand_class)]\n",
        "\n",
        "    # To Do: Use trackerID to track the same object\n",
        "    # -> could be added later on, as we only use one object per trial?\n",
        "    # -> but if we have detection of more than one target fruit, it breaks something\n",
        "    # because it will try to calculate positions from more than one bb per frame.\n",
        "    # problem so far: if detection ends for one frame, a new trackerID is assigned\n",
        "    # -> for this approach to work we would have to compare the position of the\n",
        "    # last tracked trackerID to each object with newly assigned trackerID and\n",
        "    # take the closest one, and interpolate position if there are no new trackerIDs detected.\n",
        "    # for now, we leave it out: remove class and trackingID\n",
        "    target_bbs = target_bbs[:, [0,1,3,4,5,6]]\n",
        "    hand_bbs = hand_bbs[:, [0,1,3,4,5,6]]\n",
        "\n",
        "    # 3. Interpolation: take last known bounding box positions\n",
        "    # (x_center_fruit, y_center_fruit, bbwidth_fruit, bbheight_fruit, class, trackerID, frame, filename, x_center_hand, y_center_hand, bbwidth_hand, bbheight_hand)\n",
        "    vid_input = np.zeros((frameCount, 10))\n",
        "    vid_input[:] = np.nan # use as 'no information' instead of nan, because nan is a string\n",
        "\n",
        "    #Perform interpolation for targets\n",
        "    #get x values for which detection was successful\n",
        "    x_detects = [int(target_bb[1]) for target_bb in target_bbs]\n",
        "    #get x values for which interpolation has to be performed\n",
        "    x_to_interp = [xframe for xframe in range(len(vid_input)) if xframe not in x_detects]\n",
        "\n",
        "    #get the values for successful detections\n",
        "    x_centers_targets_detec = np.asarray([frame_value[2] for frame_value in target_bbs])\n",
        "    y_centers_targets_detec = np.asarray([frame_value[3] for frame_value in target_bbs])\n",
        "    bbwidths_targets_detec = np.asarray([frame_value[4] for frame_value in target_bbs])\n",
        "    bbheights_targets_detec = np.asarray([frame_value[5] for frame_value in target_bbs])\n",
        "\n",
        "    #interpolate\n",
        "    #numpy.interp(x, xp, fp, left=None, right=None, period=None)\n",
        "    inter_x_centers_target = np.interp(x_to_interp, x_detects, x_centers_targets_detec)\n",
        "    inter_y_centers_target = np.interp(x_to_interp, x_detects, y_centers_targets_detec)\n",
        "    inter_bbwidths_target = np.interp(x_to_interp, x_detects, bbwidths_targets_detec)\n",
        "    inter_bbheights_target = np.interp(x_to_interp, x_detects, bbheights_targets_detec)\n",
        "\n",
        "    #Combine values into single array\n",
        "    #add frame and video indices for new array\n",
        "    #frame indices\n",
        "    all_frames = [frame for frame in range(len(vid_input))]\n",
        "    vid_input[all_frames,1] = range(len(vid_input))\n",
        "    #video indices\n",
        "    vid_input[all_frames,0] = i\n",
        "\n",
        "    #pseudo: vid_input[all_detec_xs][einzeln 2 bis 4 für die werte (in versch zeilen)] =  x_centers_targets_detec bis bbheights_targets_detec in den zeilen\n",
        "    vid_input[x_detects,2] = x_centers_targets_detec\n",
        "    vid_input[x_detects,3] = y_centers_targets_detec\n",
        "    vid_input[x_detects,4] = bbwidths_targets_detec\n",
        "    vid_input[x_detects,5] = bbheights_targets_detec\n",
        "\n",
        "    #und dasselbe für interpolated\n",
        "    vid_input[x_to_interp,2] = inter_x_centers_target\n",
        "    vid_input[x_to_interp,3] = inter_y_centers_target\n",
        "    vid_input[x_to_interp,4] = inter_bbwidths_target\n",
        "    vid_input[x_to_interp,5] = inter_bbheights_target\n",
        "\n",
        "\n",
        "\n",
        "    #Perform interpolation for hands\n",
        "    #get x values for which detection was successful\n",
        "    x_detects = [int(hand_bb[1]) for hand_bb in hand_bbs]\n",
        "    #get x values for which interpolation has to be performed\n",
        "    x_to_interp = [xframe for xframe in range(len(vid_input)) if xframe not in x_detects]\n",
        "\n",
        "    #get the values for successful detections\n",
        "    x_centers_hand_detec = np.asarray([frame_value[2] for frame_value in hand_bbs])\n",
        "    y_centers_hand_detec = np.asarray([frame_value[3] for frame_value in hand_bbs])\n",
        "    bbwidths_hand_detec = np.asarray([frame_value[4] for frame_value in hand_bbs])\n",
        "    bbheights_hand_detec = np.asarray([frame_value[5] for frame_value in hand_bbs])\n",
        "\n",
        "    #interpolate\n",
        "    #numpy.interp(x, xp, fp, left=None, right=None, period=None)\n",
        "    inter_x_centers_hand = np.interp(x_to_interp, x_detects, x_centers_hand_detec)\n",
        "    inter_y_centers_hand = np.interp(x_to_interp, x_detects, y_centers_hand_detec)\n",
        "    inter_bbwidths_hand = np.interp(x_to_interp, x_detects, bbwidths_hand_detec)\n",
        "    inter_bbheights_hand = np.interp(x_to_interp, x_detects, bbheights_hand_detec)\n",
        "\n",
        "    #Combine values into single array\n",
        "    #add frame and video indices for new array\n",
        "    #frame indices\n",
        "    all_frames = [frame for frame in range(len(vid_input))]\n",
        "    vid_input[all_frames,1] = range(len(vid_input))\n",
        "    #video indices\n",
        "    vid_input[all_frames,0] = i\n",
        "\n",
        "    #pseudo: vid_input[all_detec_xs][einzeln 2 bis 4 für die werte (in versch zeilen)] =  x_centers_targets_detec bis bbheights_targets_detec in den zeilen\n",
        "    vid_input[x_detects,6] = x_centers_hand_detec\n",
        "    vid_input[x_detects,7] = y_centers_hand_detec\n",
        "    vid_input[x_detects,8] = bbwidths_hand_detec\n",
        "    vid_input[x_detects,9] = bbheights_hand_detec\n",
        "\n",
        "    #und dasselbe für interpolated\n",
        "    vid_input[x_to_interp,6] = inter_x_centers_hand\n",
        "    vid_input[x_to_interp,7] = inter_y_centers_hand\n",
        "    vid_input[x_to_interp,8] = inter_bbwidths_hand\n",
        "    vid_input[x_to_interp,9] = inter_bbheights_hand\n",
        "\n",
        "    # get first frame with information of target (used for input later)\n",
        "    start_frame = 0\n",
        "    for i, frame in enumerate(vid_input[:, :6]):\n",
        "        if np.isnan(frame).any():\n",
        "            start_frame = i+1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    # 4. Center: Center on the correct bounding boxes of the target fruit\n",
        "    offsets = Center(folderpath+video, labelpath+video, vid_input[:, 2:6])\n",
        "\n",
        "    new_target_bbs = vid_input[:,2:6]\n",
        "    new_hand_bbs = vid_input[:,6:]\n",
        "\n",
        "    #Calculate new bounding boxes with offsets\n",
        "    x_offsets = offsets[:,0]\n",
        "    y_offsets = offsets[:,1]\n",
        "\n",
        "    #Adjust x\n",
        "    new_target_bbs[:,0] += x_offsets\n",
        "    new_hand_bbs[:,0] += x_offsets\n",
        "\n",
        "    #Adjust y\n",
        "    new_target_bbs[:,1] += y_offsets\n",
        "    new_hand_bbs[:,1] += y_offsets\n",
        "\n",
        "    targets_folderID = '1u6jLaX4Mb5UtQzDGtXvQPgwJIbG92YBT'\n",
        "    hands_folderID = '1901iQUf4nCOi1ZPJRuyJeEQ0MweQKlka'\n",
        "\n",
        "    #Create npz files\n",
        "    np.savez('Post_Centering_BBs_Target_Video_{}.npz'.format(i), array=new_target_bbs)\n",
        "    np.savez('Post_Centering_BBs_Hand_Video_{}.npz'.format(i), array=new_hand_bbs)\n",
        "\n",
        "    #Write npz files to drive\n",
        "    Post_Centering_BBs_Target = drive.CreateFile({'title': 'Post_Centering_BBs_Target_Video_{}.npz'.format(i), 'parents': [{'id': targets_folderID}]})\n",
        "    Post_Centering_BBs_Target.SetContentFile('Post_Centering_BBs_Target_Video_{}.npz'.format(i))\n",
        "    Post_Centering_BBs_Target.Upload()\n",
        "\n",
        "    Post_Centering_BBs_Hands = drive.CreateFile({'title': 'Post_Centering_BBs_Hand_Video_{}.npz'.format(i), 'parents': [{'id': hands_folderID}]})\n",
        "    Post_Centering_BBs_Hands.SetContentFile('Post_Centering_BBs_Hand_Video_{}.npz'.format(i))\n",
        "    Post_Centering_BBs_Hands.Upload()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}